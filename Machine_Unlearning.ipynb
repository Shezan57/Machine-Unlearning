{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34f34bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      " SECTION 1: SETUP & DEPENDENCIES\n",
      "======================================================================\n",
      "\n",
      "[+] Using device: cuda\n",
      "    GPU Name: Tesla T4\n",
      "    GPU Memory: 15.8 GB\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SECTION 1: SETUP & DEPENDENCIES\n",
    "# =============================================================================\n",
    "print(\"=\"*70)\n",
    "print(\" SECTION 1: SETUP & DEPENDENCIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# === UNCOMMENT THIS LINE IN GOOGLE COLAB ===\n",
    "!pip install transformers accelerate torch matplotlib tqdm --quiet\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\n[+] Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"    GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"    GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"[!] WARNING: No GPU detected!\")\n",
    "    print(\"    In Colab: Runtime > Change runtime type > GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36301c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      " SECTION 2: LOAD MODEL & VERIFY BASELINE KNOWLEDGE\n",
      "======================================================================\n",
      "\n",
      "[+] Loading model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b65121072154057bc8de9c523b2bc1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e12221a9b54828bfd1473bc0702425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b1be6df17c148d6aac35228935ba08d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a25f2c97b944bbcb5cafcc6ba1cf138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d4c2bdab57e4f40877b4968f2b1138a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f07e97f6bf4ebdbd9e3a2d746c1a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f8e491173a84ce0932a506397d36069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Loading frozen reference model for KL divergence...\n",
      "\n",
      "[OK] Models loaded successfully!\n",
      "     Total parameters: 1.10B\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SECTION 2: LOAD MODEL & VERIFY BASELINE\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" SECTION 2: LOAD MODEL & VERIFY BASELINE KNOWLEDGE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "print(f\"\\n[+] Loading model: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Main model - this will be modified during unlearning\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float32,  # Use float32 for stability during training\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Reference model - frozen copy for KL divergence calculation\n",
    "print(\"[+] Loading frozen reference model for KL divergence...\")\n",
    "reference_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float32,  # Match dtype\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "reference_model.eval()\n",
    "for param in reference_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(f\"\\n[OK] Models loaded successfully!\")\n",
    "print(f\"     Total parameters: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f66f2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "# Chat template tokens for TinyLlama\n",
    "SYS_START = \"<\" + \"|system|\" + \">\"\n",
    "SYS_END = \"<\" + \"/s\" + \">\"\n",
    "USER_START = \"<\" + \"|user|\" + \">\"\n",
    "ASST_START = \"<\" + \"|assistant|\" + \">\"\n",
    "\n",
    "def generate_response(gen_model, prompt, max_tokens=80):\n",
    "    \"\"\"Generate a response using TinyLlama's chat format.\"\"\"\n",
    "    formatted = f\"{SYS_START}\\nYou are a helpful assistant.{SYS_END}\\n{USER_START}\\n{prompt}{SYS_END}\\n{ASST_START}\\n\"\n",
    "    \n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(gen_model.device)\n",
    "    \n",
    "    # Ensure model is in eval mode and no gradients\n",
    "    was_training = gen_model.training\n",
    "    gen_model.eval()\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            # Clear cache before generation\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            outputs = gen_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_tokens,\n",
    "                do_sample=False,  # Use greedy decoding for stability\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                use_cache=True\n",
    "            )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "        # Extract assistant response\n",
    "        if ASST_START in response:\n",
    "            response = response.split(ASST_START)[-1]\n",
    "        if SYS_END in response:\n",
    "            response = response.split(SYS_END)[0]\n",
    "        return response.strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"[Generation error: {str(e)[:50]}]\"\n",
    "    \n",
    "    finally:\n",
    "        if was_training:\n",
    "            gen_model.train()\n",
    "\n",
    "\n",
    "def compute_loss(model_to_use, text):\n",
    "    \"\"\"\n",
    "    Compute CrossEntropy loss for generating a given text.\n",
    "    \n",
    "    This measures how well the model can predict/generate the text.\n",
    "    Higher loss = model is more \"confused\" about the text.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model_to_use.device)\n",
    "    outputs = model_to_use(**inputs, labels=inputs[\"input_ids\"])\n",
    "    return outputs.loss\n",
    "\n",
    "\n",
    "def compute_kl_divergence(current_model, ref_model, text):\n",
    "    \"\"\"\n",
    "    Compute KL Divergence between current and reference model distributions.\n",
    "    \n",
    "    KL DIVERGENCE EXPLAINED:\n",
    "    - Measures how different two probability distributions are\n",
    "    - If KL is low: models produce similar outputs\n",
    "    - If KL is high: current model has diverged significantly\n",
    "    \n",
    "    We want KL to stay LOW to prevent catastrophic forgetting.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(current_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        ref_outputs = ref_model(**inputs)\n",
    "        ref_logits = ref_outputs.logits.detach()\n",
    "    \n",
    "    current_outputs = current_model(**inputs)\n",
    "    current_logits = current_outputs.logits\n",
    "    \n",
    "    # Convert logits to probabilities\n",
    "    ref_probs = F.softmax(ref_logits, dim=-1)\n",
    "    current_log_probs = F.log_softmax(current_logits, dim=-1)\n",
    "    \n",
    "    # KL(P||Q) = sum(P * log(P/Q)) = sum(P * (log(P) - log(Q)))\n",
    "    kl_div = F.kl_div(current_log_probs, ref_probs, reduction='batchmean')\n",
    "    \n",
    "    return kl_div\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5454614e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      " SECTION 3: BASELINE VERIFICATION - Does the model know Harry Potter?\n",
      "======================================================================\n",
      "\n",
      "[+] Test Question: 'Who is Harry Potter?'\n",
      "[+] Forget Target: 'Harry Potter is a wizard in the series by J.K. Rowling.'\n",
      "[+] Retain Target: 'The sky is blue and the grass is green.'\n",
      "\n",
      "--- BASELINE RESPONSE ---\n",
      "Q: Who is Harry Potter?\n",
      "A: Harry Potter is a fictional character created by J.K. Rowling. He is a young wizard who discovers he has magical powers and is a member of the Hogwarts School of Witchcraft and Wizardry. Harry is the protagonist of the Harry Potter series, which consists of seven books and a supplementary book, Harry Potter and the\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SECTION 3: BASELINE VERIFICATION\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" SECTION 3: BASELINE VERIFICATION - Does the model know Harry Potter?\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# The question we'll use to test unlearning\n",
    "TEST_QUESTION = \"Who is Harry Potter?\"\n",
    "\n",
    "# The knowledge we want the model to FORGET\n",
    "FORGET_TARGET = \"Harry Potter is a fictional character created by J.K. Rowling.\"\n",
    "\n",
    "# Generic text to RETAIN (prevents catastrophic forgetting)\n",
    "RETAIN_TARGET = \"The sky is blue and the grass is green.\"\n",
    "\n",
    "print(f\"\\n[+] Test Question: '{TEST_QUESTION}'\")\n",
    "print(f\"[+] Forget Target: '{FORGET_TARGET}'\")\n",
    "print(f\"[+] Retain Target: '{RETAIN_TARGET}'\")\n",
    "\n",
    "print(\"\\n--- BASELINE RESPONSE ---\")\n",
    "model.eval()\n",
    "baseline_response = generate_response(model, TEST_QUESTION)\n",
    "print(f\"Q: {TEST_QUESTION}\")\n",
    "print(f\"A: {baseline_response}\")\n",
    "print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d018e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      " SECTION 4: UNLEARNING LOOP - GRADIENT ASCENT\n",
      "======================================================================\n",
      "\n",
      "[+] Hyperparameters:\n",
      "    Learning Rate: 1e-05\n",
      "    Retain Lambda: 0.5\n",
      "    Max Steps: 50\n",
      "    Eval Every: 10 steps\n",
      "\n",
      "[+] Starting Unlearning Loop...\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unlearning:   0%|          | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: device-side assert triggered\nSearch for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAcceleratorError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-274305.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mEVAL_EVERY\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mcurrent_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEST_QUESTION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-3419178027.py\u001b[0m in \u001b[0;36mgenerate_response\u001b[0;34m(gen_model, prompt, max_tokens)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         outputs = gen_model.generate(\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m         \u001b[0;31m# 9. Call generation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2564\u001b[0;31m         result = decoding_method(\n\u001b[0m\u001b[1;32m   2565\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2566\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2842\u001b[0m             \u001b[0munfinished_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munfinished_sequences\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mstopping_criteria\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2843\u001b[0;31m             \u001b[0mthis_peer_finished\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munfinished_sequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2844\u001b[0m             \u001b[0mcur_len\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAcceleratorError\u001b[0m: CUDA error: device-side assert triggered\nSearch for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SECTION 4: THE UNLEARNING LOOP (CORE SCIENCE)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" SECTION 4: UNLEARNING LOOP - GRADIENT ASCENT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\"\"\"\n",
    "THE CORE IDEA:\n",
    "--------------\n",
    "Normal training: We MINIMIZE loss to make the model BETTER at predicting text\n",
    "Unlearning:      We MAXIMIZE loss to make the model WORSE at predicting text\n",
    "\n",
    "HOW WE DO IT:\n",
    "Instead of: optimizer.step() which does theta = theta - lr * grad (descent)\n",
    "We use:     loss = -forget_loss + lambda * retain_loss\n",
    "\n",
    "By minimizing NEGATIVE forget_loss, we're actually MAXIMIZING it!\n",
    "This pushes the model away from generating the forget target.\n",
    "\n",
    "The retain_loss (KL divergence) keeps general capabilities intact.\n",
    "\"\"\"\n",
    "\n",
    "# Hyperparameters\n",
    "LEARNING_RATE = 5e-6      # Very small LR for stability\n",
    "RETAIN_LAMBDA = 1.0       # Balance between forgetting and retaining\n",
    "MAX_STEPS = 50            # Number of unlearning steps\n",
    "EVAL_EVERY = 10           # Print output every N steps\n",
    "\n",
    "print(f\"\\n[+] Hyperparameters:\")\n",
    "print(f\"    Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"    Retain Lambda: {RETAIN_LAMBDA}\")\n",
    "print(f\"    Max Steps: {MAX_STEPS}\")\n",
    "print(f\"    Eval Every: {EVAL_EVERY} steps\")\n",
    "\n",
    "# Setup optimizer\n",
    "model.train()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Tracking metrics for visualization\n",
    "history = {\n",
    "    'step': [],\n",
    "    'forget_loss': [],\n",
    "    'retain_loss': [],\n",
    "    'total_loss': []\n",
    "}\n",
    "\n",
    "print(\"\\n[+] Starting Unlearning Loop...\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for step in tqdm(range(1, MAX_STEPS + 1), desc=\"Unlearning\"):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # =========================================================================\n",
    "    # FORGET LOSS: CrossEntropy on the target we want to forget\n",
    "    # We will NEGATE this to maximize it (make model worse at this)\n",
    "    # =========================================================================\n",
    "    forget_loss = compute_loss(model, FORGET_TARGET)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # RETAIN LOSS: KL Divergence on generic text\n",
    "    # This keeps the model's general behavior similar to original\n",
    "    # We want to MINIMIZE this (keep model stable on general text)\n",
    "    # =========================================================================\n",
    "    retain_loss = compute_kl_divergence(model, reference_model, RETAIN_TARGET)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # COMBINED OBJECTIVE:\n",
    "    # Minimize: -forget_loss + lambda * retain_loss\n",
    "    # This MAXIMIZES forget_loss while MINIMIZING retain_loss\n",
    "    # =========================================================================\n",
    "    total_loss = -forget_loss + RETAIN_LAMBDA * retain_loss\n",
    "    \n",
    "    # Check for NaN/Inf before backprop\n",
    "    if torch.isnan(total_loss) or torch.isinf(total_loss):\n",
    "        print(f\"\\n[!] Warning: Loss became NaN/Inf at step {step}. Stopping.\")\n",
    "        break\n",
    "    \n",
    "    # Backpropagation\n",
    "    total_loss.backward()\n",
    "    \n",
    "    # Gradient clipping to prevent exploding gradients\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    # Record metrics\n",
    "    history['step'].append(step)\n",
    "    history['forget_loss'].append(forget_loss.item())\n",
    "    history['retain_loss'].append(retain_loss.item())\n",
    "    history['total_loss'].append(total_loss.item())\n",
    "    \n",
    "    # Periodic evaluation - only print metrics, skip generation during training\n",
    "    if step % EVAL_EVERY == 0 or step == 1:\n",
    "        print(f\"\\n[Step {step:3d}] Forget Loss: {forget_loss.item():.4f} | \"\n",
    "              f\"Retain Loss: {retain_loss.item():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"[OK] Unlearning complete!\")\n",
    "\n",
    "# Clear memory before evaluation\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a778b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 5: FINAL EVALUATION\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" SECTION 5: FINAL EVALUATION - Did the model forget Harry Potter?\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(\"\\n--- AFTER UNLEARNING ---\")\n",
    "final_response = generate_response(model, TEST_QUESTION)\n",
    "print(f\"Q: {TEST_QUESTION}\")\n",
    "print(f\"A: {final_response}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Test general capability (should still work)\n",
    "print(\"\\n--- GENERAL CAPABILITY CHECK ---\")\n",
    "general_question = \"What color is the sky?\"\n",
    "general_response = generate_response(model, general_question)\n",
    "print(f\"Q: {general_question}\")\n",
    "print(f\"A: {general_response}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00cf249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 6: SAFETY REPORT VISUALIZATION\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" SECTION 6: SAFETY REPORT - VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create dual-axis plot\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot Forget Loss (should INCREASE)\n",
    "color1 = '#FF6B6B'\n",
    "ax1.set_xlabel('Training Step', fontsize=12)\n",
    "ax1.set_ylabel('Forget Loss (CrossEntropy)', color=color1, fontsize=12)\n",
    "line1 = ax1.plot(history['step'], history['forget_loss'], \n",
    "                  color=color1, linewidth=2, marker='o', markersize=4, \n",
    "                  label='Forget Loss (should increase)')\n",
    "ax1.tick_params(axis='y', labelcolor=color1)\n",
    "\n",
    "# Create second y-axis for Retain Loss (should stay FLAT)\n",
    "ax2 = ax1.twinx()\n",
    "color2 = '#4ECDC4'\n",
    "ax2.set_ylabel('Retain Loss (KL Divergence)', color=color2, fontsize=12)\n",
    "line2 = ax2.plot(history['step'], history['retain_loss'], \n",
    "                  color=color2, linewidth=2, marker='s', markersize=4,\n",
    "                  label='Retain Loss (should stay flat)')\n",
    "ax2.tick_params(axis='y', labelcolor=color2)\n",
    "\n",
    "# Title and legend\n",
    "plt.title('Machine Unlearning: Targeted Amnesia Progress\\n' + \n",
    "          'Goal: Increase Forget Loss while keeping Retain Loss stable', \n",
    "          fontsize=14, fontweight='bold')\n",
    "\n",
    "# Combined legend\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='center right', fontsize=10)\n",
    "\n",
    "# Grid\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('unlearning_progress.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n[OK] Visualization saved as 'unlearning_progress.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d611bbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 7: SUMMARY\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" SUMMARY: MACHINE UNLEARNING RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "BEFORE UNLEARNING:\n",
    "  Q: {TEST_QUESTION}\n",
    "  A: {baseline_response[:80]}...\n",
    "\n",
    "AFTER UNLEARNING:\n",
    "  Q: {TEST_QUESTION}\n",
    "  A: {final_response[:80]}...\n",
    "\n",
    "METRICS:\n",
    "  Initial Forget Loss: {history['forget_loss'][0]:.4f}\n",
    "  Final Forget Loss:   {history['forget_loss'][-1]:.4f}\n",
    "  Change:              {history['forget_loss'][-1] - history['forget_loss'][0]:+.4f}\n",
    "\n",
    "  Initial Retain Loss: {history['retain_loss'][0]:.4f}\n",
    "  Final Retain Loss:   {history['retain_loss'][-1]:.4f}\n",
    "  Change:              {history['retain_loss'][-1] - history['retain_loss'][0]:+.4f}\n",
    "\n",
    "INTERPRETATION:\n",
    "  - If Forget Loss INCREASED: Model is \"forgetting\" the target knowledge\n",
    "  - If Retain Loss stayed STABLE: General capabilities are preserved\n",
    "  - SUCCESS = Targeted forgetting without catastrophic forgetting!\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\" END OF MACHINE UNLEARNING PROTOTYPE\")\n",
    "print(\"=\"*70)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
